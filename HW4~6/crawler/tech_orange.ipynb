{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # 爬蟲\n"," 在科技報橘 TechOrange 網站爬五個產業的新聞各80篇，包含電商、區塊鍊、金融、行銷和旅遊，最後分別將爬蟲結果存成五個檔案\r\n\r\n"," [爬蟲結果](https://github.com/ChiaYi-LIN/1072-CSX4001-B04704016/edit/master/HW4~6/crawler/data)"],"metadata":{}},{"source":["import sys\n","import pickle\n","import requests\n","from datetime import datetime\n","from bs4 import BeautifulSoup\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Main function. In oreder to iterate through all pages on the website.\n","def get_data_from_news_pages(from_page, to_page, search_word):\n","    url_1 = \"https://buzzorange.com/techorange/page/\"\n","    url_2 = \"/?s=\"\n","    print(\"page_number from {} to {}\" .format(from_page, to_page))\n","    data = []\n","    for page_number in range(from_page, to_page + 1):\n","        print(\"page_number: {}\" .format(page_number))\n","        data = data + get_news_from_each_page(url_1 + str(page_number) + url_2 + search_word)\n","    \n","    print(\"done\")\n","    return(data)\n","\n","# Get all news of one page\n","def get_news_from_each_page(url):\n","    r = requests.get(url)\n","    r.encoding = \"UTF-8\"\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","    news_blocks = soup.find_all(\"article\")\n","    \n","    news = []\n","    for each_news in news_blocks:\n","        try:\n","            news_info = get_news_info(each_news)\n","            # print(get_title(each_news))\n","        except:\n","            pass\n","        else:\n","            news.append(news_info)\n","    return(news)\n","\n","# Get all information, including date, title, share times, link and content, of a news of one page. Then, combine the information as a dictionary object.\n","def get_news_info(each_news):\n","    date  = get_date(each_news)\n","    title = get_title(each_news)\n","    share = get_share(each_news)\n","    link  = get_link(each_news)\n","    content = get_content(link)\n","    info = {\n","        \"date\" : date,\n","        \"title\" : title,\n","        \"share\" : share,\n","        \"link\" : link,\n","        \"content\" : content\n","        }\n","    return(info)\n","\n","# Get the published date of a news\n","def get_date(news_block_node):\n","    date_string = news_block_node.find(\"time\", class_=\"entry-date\").text\n","    return(datetime.strptime(date_string, \"%Y/%m/%d\").strftime(\"%Y-%m-%d\"))\n","\n","# Get the title of a news\n","def get_title(news_block_node):\n","    return news_block_node.find(\"h4\", class_=\"entry-title\").a.text\n","\n","# Get the share times of a news\n","def get_share(news_block_node):\n","    return news_block_node.find(\"span\", class_=\"shareCount\").text\n","\n","# Get the link of a news\n","def get_link(news_block_node):\n","    return news_block_node.find(\"h4\", class_=\"entry-title\").a.get(\"href\")\n","\n","# Get the contnent of a news\n","def get_content(link):\n","    r = requests.get(link)\n","    r.encoding = \"UTF-8\"\n","    article = \"\"\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","    article_block = soup.find(\"div\", class_=\"entry-content\")\n","    for article_node in article_block.find_all(\"p\"):\n","        if article_node.find(\"a\"):\n","            continue\n","        article += article_node.text\n","    # print(article)\n","    return article.replace(\"\\n\", \"\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Scrap through indicated industries\n","for industry_to_search in [\"電商\", \"區塊鍊\", \"金融\", \"行銷\", \"旅遊\"]:\n","    data = get_data_from_news_pages(from_page=1, to_page=10, search_word=industry_to_search)\n","    sys.setrecursionlimit(100000)\n","    with open(\"./crawler/data/tech_orange_\" + industry_to_search + \".pkl\", \"wb\") as handle:\n","        pickle.dump(data, handle)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
